# Um Bilh√£o de Linhas: Desafio de Processamento de Dados com Python

## Introdu√ß√£o

O objetivo deste projeto √© demonstrar como processar eficientemente um arquivo de dados massivo contendo 1 bilh√£o de linhas (~14GB), especificamente para calcular estat√≠sticas (Incluindo agrega√ß√£o e ordena√ß√£o que s√£o opera√ß√µes pesadas) utilizando Python. 

Este desafio foi inspirado no [The One Billion Row Challenge](https://github.com/gunnarmorling/1brc), originalmente proposto para Java.

O arquivo de dados consiste em medi√ß√µes de temperatura de v√°rias esta√ß√µes meteorol√≥gicas. Cada registro segue o formato `<string: nome da esta√ß√£o>;<double: medi√ß√£o>`, com a temperatura sendo apresentada com precis√£o de uma casa decimal.

Aqui est√£o dez linhas de exemplo do arquivo:

```
Hamburg;12.0
Bulawayo;8.9
Palembang;38.8
St. Johns;15.2
Cracow;12.6
Bridgetown;26.9
Istanbul;6.2
Roseau;34.4
Conakry;31.2
Istanbul;23.0
```

O desafio √© desenvolver um programa Python capaz de ler esse arquivo e calcular a temperatura m√≠nima, m√©dia (arredondada para uma casa decimal) e m√°xima para cada esta√ß√£o, exibindo os resultados em uma tabela ordenada por nome da esta√ß√£o.

| station      | min_temperature | mean_temperature | max_temperature |
|--------------|-----------------|------------------|-----------------|
| Abha         | -31.1           | 18.0             | 66.5            |
| Abidjan      | -25.9           | 26.0             | 74.6            |
| Albuquerque  | -35.0           | 14.0             | 61.9            |
| Alexandra    | -40.1           | 11.0             | 67.9            |
| ...          | ...             | ...              | ...             |
| Yangon       | -23.6           | 27.5             | 77.3            |
| Yaound√©      | -26.2           | 23.8             | 73.4            |
| Yellowknife  | -53.4           | -4.3             | 46.7            |
| Z√ºrich       | -42.0           | 9.3              | 63.6            |
| √úr√ºmqi       | -42.1           | 7.4              | 56.7            |
| ƒ∞zmir        | -34.4           | 17.9             | 67.9            |

Este desafio ser√° em torno das bibliotecas:

* Polars: `0.20.3`
* DuckDB: `0.10.0`
* Dask[complete]: `^2024.2.0`

## Resultados dos Testes

Os testes foram realizados em um laptop equipado com um processador Intel i5 Gen8 e 8Gb RAM. As implementa√ß√µes utilizaram abordagens puramente Python, Pandas, Dask, Polars e DuckDB. Os resultados de tempo de execu√ß√£o para processar o arquivo de 1 bilh√£o de linhas s√£o apresentados abaixo:

| Implementa√ß√£o | Tempo |
| --- | --- |
| Python | 48 minutos |
| Python + Pandas | 759.77 sec |
| Python + Dask | 155.62 sec  |
| Python + Polars | 77.74 sec |
| Python + Duckdb | 52.31 sec |

## Conclus√£o

Este desafio destacou claramente a efic√°cia de diversas bibliotecas Python na manipula√ß√£o de grandes volumes de dados. M√©todos tradicionais Python puro (48 minutos) e at√© mesmo o Pandas (12 minutos) demandaram uma s√©rie de t√°ticas para implementar o processamento em "lotes", enquanto bibliotecas como Dask, Polars e DuckDB provaram ser excepcionalmente eficazes, requerendo menos linhas de c√≥digo e  sua capacidade inerente de distribuir os dados em "lotes em streaming" de maneira mais eficiente. O DuckDB se sobressaiu, alcan√ßando o menor tempo de execu√ß√£o gra√ßas √† sua estrat√©gia de execu√ß√£o e processamento de dados.

Esses resultados enfatizam a import√¢ncia de selecionar a ferramenta adequada para an√°lise de dados em larga escala, demonstrando que Python, com as bibliotecas certas, √© uma escolha poderosa para enfrentar desafios de big data.

## Como Executar

Para executar este projeto e reproduzir os resultados:

1. Clone esse reposit√≥rio
2. Definir a versao do Python usando o `pyenv local 3.12.1`
2. `poetry env use 3.12.1`, `poetry install --no-root` e `poetry lock --no-update`
3. Execute o comando `python src/create_measurements.py` para gerar o arquivo de teste
4. Tenha paci√™ncia e v√° fazer um caf√©, vai demorar uns 15 minutos para gerar o arquivo
5. Execute os scripts `poetry run python src/using_python.py`, `poetry run python src/using_pandas.py`, `poetry run python src/using_dask.py`, `poetry run python src/using_polars.py` e `poetry run python src/using_duckdb.py` atrav√©s de um terminal ou ambiente de desenvolvimento que suporte Python.

Este projeto destaca a versatilidade do ecossistema Python para tarefas de processamento de dados, oferecendo valiosas li√ß√µes sobre escolha de ferramentas para an√°lises em grande escala.

## Caracter√≠sticas de Cada Biblioteca

### Pandas
Como funciona:
* Pandas usa NumPy por baixo dos panos, armazenando os dados em arrays na RAM
* Cada coluna √© um Series, que √© basicamente um array tipado
* Todas as opera√ß√µes s√£o monothreaded (rodam em um n√∫cleo apenas)
* N√£o faz lazy evaluation: tudo roda imediatamente.

Ideal para: 
* Pequenos dados (cabem na RAM)

Motivo:
* Simples, leve e direto. √â perfeito para an√°lises explorat√≥rias, prot√≥tipos r√°pidos e pequenos datasets
* Sua API √© muito madura e intuitiva, o que facilita a vida no dia a dia

Limite t√©cnico:
* Dependendo de cada m√°quina, mas se voc√™ tem 8Gb RAM e se seu DataFrame tem ~500k a 1M de linhas e voc√™ tenta um groupby, ele pode travar ou estourar mem√≥ria. Observa√ß√£o: Lembrando que tamb√©m existe a quest√£o de quantidades de colunas do DataFrame

### DuckDB
Como funciona:
* √â um motor de banco de dados OLAP embutido, otimizado para consultas anal√≠ticas colunarizadas
* L√™ e escreve formato Parquet diretamente, com scan incremental (sem carregar tudo na RAM)
* Usa multi-threading com query planner e otimizador de execu√ß√£o como bancos de dados s√©rios
* Suporta SQL ANSI, inclusive subqueries, CTEs, joins complexos etc

Ideal para:
OLAP com SQL, e dados grandes que n√£o cabem na mem√≥ria

Motivo:
* Pode rodar queries em datasets de centenas de milh√µes de linhas sem carregar tudo na RAM
* Excelente para times acostumados com SQL, e quando o dado est√° armazenado em arquivos como .parquet, .csv, ou at√© no Amazon S3

Limite t√©cnico:
* N√£o tem API pandas-style. √â SQL puro (ou .df() para converter o resultado), ent√£o pode n√£o ser ideal se voc√™ quer manipular os dados linha por linha ou fazer coisas muito customizadas

### Polars
Como funciona:
* Feito em Rust, super r√°pido e seguro, com bindings para Python
* Usa executores vetorizados (colunas processadas em blocos) e multi-threading nativo
* Pode operar em dois modos:
** Eager (modo imediato) ‚Äî parecido com Pandas
**Lazy (modo pregui√ßoso) ‚Äî compila todo o pipeline de opera√ß√µes e executa s√≥ no final, otimizando tudo (como um query planner)

Ideal para
* Grandes dados em mem√≥ria e processamento super r√°pido

Motivo:
* √â muito mais eficiente que Pandas, especialmente com filtros, joins e agrega√ß√µes complexas
* Lazy evaluation permite otimiza√ß√µes autom√°ticas (ex: elimina colunas n√£o usadas)

Limite t√©cnico:
* Ainda n√£o t√£o ‚Äúamig√°vel‚Äù quanto Pandas para quem est√° come√ßando.
* Algumas opera√ß√µes ainda est√£o sendo desenvolvidas (tipo merge complexos ou fun√ß√µes personalizadas)

### Dask
Como funciona:
* Cria gr√°ficos de tarefas paralelas (DAGs) para opera√ß√µes com dados
* Divide grandes DataFrames em parti√ß√µes menores e processa cada uma em paralelo, podendo escalar para clusters
* API √© parecida com Pandas, mas as opera√ß√µes s√£o lazy, ou seja, n√£o executam at√© o .compute()

Ideal para: 
* Escalar an√°lise de dados para al√©m da RAM, ou para m√∫ltiplas m√°quinas

Motivo:
* Voc√™ pode trabalhar com dados maiores que a RAM, j√° que ele processa em blocos
* Tamb√©m pode conectar com Hadoop/Spark, usar com AWS/GCP, etc
* √ìtimo para pipelines e automa√ß√µes

üîé Limite t√©cnico:
* Nem toda opera√ß√£o √© compat√≠vel com Pandas 100%
* Performance pode ser menor que Polars ou DuckDB em casos simples
* Curva de aprendizado maior